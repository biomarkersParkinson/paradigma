{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gait analysis\n",
    "This tutorial showcases the high-level functions composing the gait pipeline. Before following along, make sure all data preparation steps have been followed in the [Data preparation tutorial](https://biomarkersparkinson.github.io/paradigma/tutorials/_static/data_preparation.html). \n",
    "\n",
    "In this tutorial, we use two days of data from a participant of the Personalized Parkinson Project to demonstrate the functionalities. Since ParaDigMa expects contiguous time series, the collected data was stored in two segments each with contiguous timestamps. Per segment, we load the data and perform the following steps:\n",
    "1. Data preprocessing\n",
    "2. Gait feature extraction\n",
    "3. Gait detection\n",
    "4. Arm activity feature extraction\n",
    "5. Filtering gait\n",
    "6. Arm swing quantification\n",
    "\n",
    "We then combine the output of the different raw data segments for the final step:\n",
    "\n",
    "7. Aggregation\n",
    "\n",
    "To run the complete gait pipeline, a prerequisite is to have both accelerometer and gyroscope data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from importlib.resources import files\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tsdf\n",
    "\n",
    "from paradigma.classification import ClassifierPackage\n",
    "from paradigma.config import GaitConfig, IMUConfig\n",
    "from paradigma.constants import DataColumns\n",
    "from paradigma.pipelines.gait_pipeline import (\n",
    "    aggregate_arm_swing_params,\n",
    "    detect_gait,\n",
    "    extract_arm_activity_features,\n",
    "    extract_gait_features,\n",
    "    filter_gait,\n",
    "    quantify_arm_swing,\n",
    ")\n",
    "from paradigma.preprocessing import preprocess_imu_data\n",
    "from paradigma.util import (\n",
    "    load_tsdf_dataframe,\n",
    "    merge_predictions_with_timestamps,\n",
    "    write_df_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "Here, we start by loading a single contiguous time series (segment), for which we continue running steps 1-6. [Below](#multiple_segments_cell) we show how to run these steps for multiple raw data segments.\n",
    "\n",
    "We use the internally developed `TSDF` ([documentation](https://biomarkersparkinson.github.io/tsdf/)) to load and store data [[1](https://arxiv.org/abs/2211.11294)]. Depending on the file extension of your time series data, examples of other Python functions for loading the data into memory include:\n",
    "- _.csv_: `pandas.read_csv()` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html))\n",
    "- _.json_: `json.load()` ([documentation](https://docs.python.org/3/library/json.html#json.load))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to where the prepared data is saved and load the data.\n",
    "# Note: the test data is stored in TSDF, but you can load your data in your own way\n",
    "path_to_data =  Path('../../example_data/verily')\n",
    "path_to_prepared_data = path_to_data / 'imu'\n",
    "\n",
    "raw_data_segment_nr  = '0001'\n",
    "\n",
    "# Load the data from the file\n",
    "df_imu, metadata_time, metadata_values = load_tsdf_dataframe(\n",
    "    path_to_data=path_to_prepared_data,\n",
    "    prefix=f'IMU_segment{raw_data_segment_nr}'\n",
    ")\n",
    "\n",
    "df_imu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Preprocess data\n",
    "The function [`preprocess_imu_data`](https://biomarkersparkinson.github.io/paradigma/autoapi/paradigma/preprocessing/index.html#paradigma.preprocessing.preprocess_imu_data) in the cell below runs all necessary preprocessing steps. It requires the loaded dataframe, a configuration object [`config`](https://biomarkersparkinson.github.io/paradigma/autoapi/paradigma/config/index.html) specifying parameters used for preprocessing, and a selection of sensors. For the sensors, options include `'accelerometer'`, `'gyroscope'`, or `'both'`.  If the difference between timestamps is larger than a specified tolerance (`config.tolerance`, in seconds), it will return an error that the timestamps are not contiguous. If you still want to process the data in this case, you can create segments from discontiguous samples using the function [`create_segments`](https://biomarkersparkinson.github.io/paradigma/autoapi/paradigma/segmenting/index.html#paradigma.segmenting.create_segments) and analyze these segments consecutively as shown in [here](#multiple_segments_cell).\n",
    "\n",
    "The function [`preprocess_imu_data`](https://biomarkersparkinson.github.io/paradigma/autoapi/paradigma/preprocessing/index.html#paradigma.preprocessing.preprocess_imu_data) processes the data as follows:\n",
    "1. Resample the data to ensure uniformly distributed sampling rate.\n",
    "2. Apply filtering to separate the gravity component from the accelerometer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = IMUConfig()\n",
    "\n",
    "df_preprocessed = preprocess_imu_data(\n",
    "    df=df_imu,\n",
    "    config=config,\n",
    "    sensor='both',\n",
    "    watch_side='left',\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"The dataset of {df_preprocessed.shape[0] / config.sampling_frequency} seconds \"\n",
    "    f\"is automatically resampled to {config.resampling_frequency} Hz.\"\n",
    ")\n",
    "print(\n",
    "    f\"The tolerance for checking contiguous timestamps is set \"\n",
    "    f\"to {config.tolerance:.3f} seconds.\"\n",
    ")\n",
    "df_preprocessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting dataframe shown above contains uniformly distributed timestamps with corresponding accelerometer and gyroscope values. Note the for accelerometer values, the following notation is used: \n",
    "- `accelerometer_x`: the accelerometer signal after filtering out the gravitational component\n",
    "- `accelerometer_x_grav`: the gravitational component of the accelerometer signal\n",
    "\n",
    "The accelerometer data is retained and used to compute gravity-related features for the classification tasks, because the gravity is informative of the position of the arm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract gait features\n",
    "With the data uniformly resampled and the gravitional component separated from the accelerometer signal, features can be extracted from the time series data. This step does not require gyroscope data. To extract the features, the pipeline executes the following steps:\n",
    "- Use overlapping windows to group timestamps\n",
    "- Extract temporal features\n",
    "- Use Fast Fourier Transform the transform the windowed data into the spectral domain\n",
    "- Extract spectral features\n",
    "- Combine both temporal and spectral features into a final dataframe\n",
    "\n",
    "These steps are encapsulated in [`extract_gait_features`](https://biomarkersparkinson.github.io/paradigma/autoapi/paradigma/pipelines/gait_pipeline/index.html#paradigma.pipelines.gait_pipeline.extract_gait_features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set column names: replace DataColumn.* with your actual column names.\n",
    "# It is only necessary to set the columns that are present in your data, and\n",
    "# only if they differ from the default names defined in DataColumns.\n",
    "column_mapping = {\n",
    "    'TIME': DataColumns.TIME,\n",
    "    'ACCELEROMETER_X': DataColumns.ACCELEROMETER_X,\n",
    "    'ACCELEROMETER_Y': DataColumns.ACCELEROMETER_Y,\n",
    "    'ACCELEROMETER_Z': DataColumns.ACCELEROMETER_Z,\n",
    "    'GYROSCOPE_X': DataColumns.GYROSCOPE_X,\n",
    "    'GYROSCOPE_Y': DataColumns.GYROSCOPE_Y,\n",
    "    'GYROSCOPE_Z': DataColumns.GYROSCOPE_Z,\n",
    "}\n",
    "\n",
    "config = GaitConfig(step='gait', column_mapping=column_mapping)\n",
    "\n",
    "df_gait = extract_gait_features(\n",
    "    df=df_preprocessed,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"A total of {df_gait.shape[1]-1} features have been extracted from \"\n",
    "    f\"{df_gait.shape[0]} {config.window_length_s}-second windows with \"\n",
    "    f\"{config.window_length_s-config.window_step_length_s} seconds overlap.\"\n",
    ")\n",
    "df_gait.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in this dataframe corresponds to a single window, with the window length and overlap set in the [`config`](https://biomarkersparkinson.github.io/paradigma/autoapi/paradigma/config/index.html) object. Note that the `time` column has a 1-second interval instead of the 10-millisecond interval before, as it now represents the starting time of the window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Gait detection\n",
    "For classification, ParaDigMa uses so-called Classifier Packages which contain a classifier, classification threshold, and a feature scaler as attributes ([documentation](https://biomarkersparkinson.github.io/paradigma/autoapi/paradigma/classification/index.html#paradigma.classification.ClassifierPackage)). The classifier is a [random forest](https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestClassifier.html) trained on a dataset of people with PD performing a wide range of activities in free-living conditions: [The Parkinson@Home Validation Study](https://pmc.ncbi.nlm.nih.gov/articles/PMC7584982/). The classification threshold was set to limit the amount of false-positive predictions in the original study, i.e., to limit non-gait to be predicted as gait. The classification threshold can be changed by setting `clf_package.threshold` to a different float value. The feature scaler was similarly fitted on the original dataset, ensuring the features are within expected confined spaces to make reliable predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the classifier package\n",
    "classifier_package_filename = 'gait_detection_clf_package.pkl'\n",
    "full_path_to_classifier_package = (\n",
    "    files('paradigma')\n",
    "    / 'assets'\n",
    "    / classifier_package_filename\n",
    ")\n",
    "\n",
    "# Load the classifier package\n",
    "clf_package_detection = ClassifierPackage.load(full_path_to_classifier_package)\n",
    "gait_threshold = clf_package_detection.threshold\n",
    "\n",
    "# Detecting gait returns the probability of gait for each window, which is\n",
    "# concatenated to the original dataframe\n",
    "df_gait[DataColumns.PRED_GAIT_PROBA] = detect_gait(\n",
    "    df=df_gait,\n",
    "    clf_package=clf_package_detection\n",
    ")\n",
    "\n",
    "n_windows = df_gait.shape[0]\n",
    "n_predictions_gait = df_gait.loc[\n",
    "    df_gait[DataColumns.PRED_GAIT_PROBA] >= gait_threshold\n",
    "].shape[0]\n",
    "perc_predictions_gait = round(100 * n_predictions_gait / n_windows, 1)\n",
    "n_predictions_non_gait = df_gait.loc[\n",
    "    df_gait[DataColumns.PRED_GAIT_PROBA] < gait_threshold\n",
    "].shape[0]\n",
    "perc_predictions_non_gait = round(100 * n_predictions_non_gait / n_windows, 1)\n",
    "\n",
    "print(\n",
    "    f\"Out of {n_windows} windows, {n_predictions_gait} \"\n",
    "    f\"({perc_predictions_gait}%) \\n\"\n",
    "    f\"were predicted as gait, and {n_predictions_non_gait} \"\n",
    "    f\"({perc_predictions_non_gait}%) \\n\"\n",
    "    f\"as non-gait.\"\n",
    ")\n",
    "\n",
    "# Only the time and the predicted gait probability are shown, but the\n",
    "# dataframe also contains the extracted features\n",
    "df_gait[[config.time_colname, DataColumns.PRED_GAIT_PROBA]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store as TSDF\n",
    "The predicted probabilities (and optionally other features) can be stored and loaded in TSDF as demonstrated below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 'path_to_data' to the directory where you want to save the data\n",
    "metadata_time_store = tsdf.TSDFMetadata(\n",
    "    metadata_time.get_plain_tsdf_dict_copy(),\n",
    "    path_to_data\n",
    ")\n",
    "metadata_values_store = tsdf.TSDFMetadata(\n",
    "    metadata_values.get_plain_tsdf_dict_copy(),\n",
    "    path_to_data\n",
    ")\n",
    "\n",
    "# Select the columns to be saved\n",
    "metadata_time_store.channels = [config.time_colname]\n",
    "metadata_values_store.channels = [DataColumns.PRED_GAIT_PROBA]\n",
    "\n",
    "# Set the units\n",
    "metadata_time_store.units = ['Relative seconds']\n",
    "metadata_values_store.units = ['Unitless']\n",
    "metadata_time_store.data_type = float\n",
    "metadata_values_store.data_type = float\n",
    "\n",
    "# Set the filenames\n",
    "meta_store_filename = f'segment{raw_data_segment_nr}_meta.json'\n",
    "values_store_filename = meta_store_filename.replace('_meta.json', '_values.bin')\n",
    "time_store_filename = meta_store_filename.replace('_meta.json', '_time.bin')\n",
    "\n",
    "metadata_values_store.file_name = values_store_filename\n",
    "metadata_time_store.file_name = time_store_filename\n",
    "\n",
    "write_df_data(metadata_time_store, metadata_values_store, path_to_data,\n",
    "              meta_store_filename, df_gait)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gait, _, _ = load_tsdf_dataframe(\n",
    "    path_to_data,\n",
    "    prefix=f'segment{raw_data_segment_nr}'\n",
    ")\n",
    "df_gait.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the `time` column indicates the start time of the window. Therefore, it can be observed that probabilities are predicted of overlapping windows, and not of individual timestamps. The function [`merge_timestamps_with_predictions`](https://biomarkersparkinson.github.io/paradigma/autoapi/paradigma/util/index.html#paradigma.util.merge_predictions_with_timestamps) can be used to retrieve predicted probabilities per timestamp by aggregating the predicted probabilities of overlapping windows. This function is included in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Arm activity feature extraction\n",
    "The extraction of arm swing features is similar to the extraction of gait features, but we use a different window length and step length (`config.window_length_s`, `config.window_step_length_s`) to distinguish between gait segments with and without other arm activities. Therefore, the following steps are conducted sequentially by [`extract_arm_activity_features`](https://biomarkersparkinson.github.io/paradigma/autoapi/paradigma/pipelines/gait_pipeline/index.html#paradigma.pipelines.gait_pipeline.extract_arm_activity_features):\n",
    "- Start with the preprocessed data of step 1\n",
    "- Merge the gait predictions into the preprocessed data\n",
    "- Discard predicted non-gait activities\n",
    "- Create windows of the time series data and extract features\n",
    "\n",
    "But, first, the gait predictions should be merged with the preprocessed time series data, such that individual timestamps have a corresponding probability of gait. The function [`extract_arm_activity_features`](https://biomarkersparkinson.github.io/paradigma/autoapi/paradigma/pipelines/gait_pipeline/index.html#paradigma.pipelines.gait_pipeline.extract_arm_activity_features) expects a time series dataframe of predicted gait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge gait predictions into timeseries data\n",
    "if not any(df_gait[DataColumns.PRED_GAIT_PROBA] >= clf_package_detection.threshold):\n",
    "    raise ValueError(\"No gait detected in the input data.\")\n",
    "\n",
    "gait_preprocessing_config = GaitConfig(step='gait')\n",
    "\n",
    "df = merge_predictions_with_timestamps(\n",
    "    df_ts=df_preprocessed,\n",
    "    df_predictions=df_gait,\n",
    "    pred_proba_colname=DataColumns.PRED_GAIT_PROBA,\n",
    "    window_length_s=gait_preprocessing_config.window_length_s,\n",
    "    fs=gait_preprocessing_config.sampling_frequency\n",
    ")\n",
    "\n",
    "# Add a column for predicted gait based on a fitted threshold\n",
    "df[DataColumns.PRED_GAIT] = (\n",
    "    df[DataColumns.PRED_GAIT_PROBA] >= clf_package_detection.threshold\n",
    ").astype(int)\n",
    "\n",
    "# Filter the DataFrame to only include predicted gait (1)\n",
    "df = df.loc[df[DataColumns.PRED_GAIT]==1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GaitConfig(step='arm_activity')\n",
    "\n",
    "df_arm = extract_arm_activity_features(\n",
    "    df=df,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"A total of {df_arm.shape[1] - 1} features have been extracted \"\n",
    "    f\"from {df_arm.shape[0]} {config.window_length_s}-second windows \"\n",
    "    f\"with {config.window_length_s - config.window_step_length_s} seconds overlap.\"\n",
    ")\n",
    "df_arm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features extracted are similar to the features extracted for gait detection, but the gyroscope has been added to extract additional MFCCs of this sensor. The gyroscope (measuring angular velocity) is relevant to distinguish between arm activities. Also note that the `time` column no longer starts at 0, since the first timestamps were predicted as non-gait and therefore discarded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Filtering gait\n",
    "This classification task is similar to gait detection, although it uses a different classification object. The trained classifier is a logistic regression, similarly trained on the dataset of the [Parkinson@Home Validation Study](https://pmc.ncbi.nlm.nih.gov/articles/PMC7584982/). Filtering gait is the process of detecting and removing gait segments containing other arm activities. This is an important process since individuals entertain a wide array of arm activities during gait: having hands in pockets, holding a dog leash, or carrying a plate to the kitchen. We trained a classifier to detect these other arm activities during gait, enabling accurate estimations of the arm swing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the classifier package\n",
    "classifier_package_filename = 'gait_filtering_clf_package.pkl'\n",
    "full_path_to_classifier_package = (\n",
    "    files('paradigma')\n",
    "    / 'assets'\n",
    "    / classifier_package_filename\n",
    ")\n",
    "\n",
    "# Load the classifier package\n",
    "clf_package_filtering = ClassifierPackage.load(full_path_to_classifier_package)\n",
    "filt_threshold = clf_package_filtering.threshold\n",
    "\n",
    "# Detecting no_other_arm_activity returns the probability of\n",
    "# no_other_arm_activity for each window, which is concatenated to\n",
    "# the original dataframe\n",
    "df_arm[DataColumns.PRED_NO_OTHER_ARM_ACTIVITY_PROBA] = filter_gait(\n",
    "    df=df_arm,\n",
    "    clf_package=clf_package_filtering\n",
    ")\n",
    "\n",
    "\n",
    "n_windows = df_arm.shape[0]\n",
    "n_pred_no_other_arm_act = df_arm.loc[\n",
    "    df_arm[DataColumns.PRED_NO_OTHER_ARM_ACTIVITY_PROBA] >= filt_threshold\n",
    "].shape[0]\n",
    "perc_no_other_arm_activity = round(\n",
    "    100 * n_pred_no_other_arm_act / n_windows,\n",
    "    1\n",
    ")\n",
    "n_pred_other_arm_act = df_arm.loc[\n",
    "    df_arm[DataColumns.PRED_NO_OTHER_ARM_ACTIVITY_PROBA] < filt_threshold\n",
    "].shape[0]\n",
    "perc_other_arm_activity = round(\n",
    "    100 * n_pred_other_arm_act / n_windows,\n",
    "    1\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Out of {n_windows} windows, {n_pred_no_other_arm_act} \"\n",
    "    f\"({perc_no_other_arm_activity}%) were predicted as no_other_arm_activity, \"\n",
    "    f\"and {n_pred_other_arm_act} ({perc_other_arm_activity}%) as other_arm_activity.\"\n",
    ")\n",
    "\n",
    "# Only the time and predicted probabilities are shown,\n",
    "# but the dataframe also contains the extracted features\n",
    "df_arm[[config.time_colname, DataColumns.PRED_NO_OTHER_ARM_ACTIVITY_PROBA]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Arm swing quantification\n",
    "\n",
    "**Important:** As of version 1.1.0, `quantify_arm_swing()` now returns **two dictionaries** instead of a DataFrame and dict:\n",
    "- First dict: quantified arm swing parameters with keys `'filtered'` and `'unfiltered'`\n",
    "  - `'filtered'`: DataFrame with arm swings from clean gait only (no other arm activities)\n",
    "  - `'unfiltered'`: DataFrame with arm swings from all gait segments\n",
    "- Second dict: gait segment metadata with keys `'filtered'` and `'unfiltered'`\n",
    "\n",
    "This allows analysis of arm swing with and without filtering for other arm activities.\n",
    "\n",
    "The next step is to extract arm swing estimates from the predicted gait segments. The `filtered` parameter is now deprecated but still functional for backward compatibility:\n",
    "- When `filtered=True`: Returns results in the old format (single DataFrame for filtered gait)\n",
    "- When using the new format: Both filtered and unfiltered results are returned together\n",
    "\n",
    "Specifically, the range of motion (`'range_of_motion'`) and peak angular velocity (`'peak_velocity'`) are extracted.\n",
    "\n",
    "This step creates gait segments based on consecutively predicted gait windows. A new gait segment is created if the gap between consecutive gait predictions exceeds `config.max_segment_gap_s`. Furthermore, a gait segment is considered valid if it is of at minimum length `config.min_segment_length_s`.\n",
    "\n",
    "But, first, similar to the step of extracting arm activity features, the predictions of the previous step should be merged with the preprocessed time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge arm activity predictions into timeseries data\n",
    "\n",
    "if not any(\n",
    "    df_arm[DataColumns.PRED_NO_OTHER_ARM_ACTIVITY_PROBA] >= filt_threshold\n",
    "):\n",
    "    raise ValueError(\n",
    "        \"No gait without other arm activities detected in the input data.\"\n",
    "    )\n",
    "\n",
    "config = GaitConfig(step='arm_activity')\n",
    "\n",
    "df = merge_predictions_with_timestamps(\n",
    "    df_ts=df_preprocessed,\n",
    "    df_predictions=df_arm,\n",
    "    pred_proba_colname=DataColumns.PRED_NO_OTHER_ARM_ACTIVITY_PROBA,\n",
    "    window_length_s=config.window_length_s,\n",
    "    fs=config.sampling_frequency\n",
    ")\n",
    "\n",
    "# Add a column for predicted gait based on a fitted threshold\n",
    "df[DataColumns.PRED_NO_OTHER_ARM_ACTIVITY] = (\n",
    "    df[DataColumns.PRED_NO_OTHER_ARM_ACTIVITY_PROBA] >= filt_threshold\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True to quantify arm swing based on the filtered gait segments, and\n",
    "# False to quantify arm swing based on all gait segments\n",
    "filtered = True\n",
    "\n",
    "if filtered:\n",
    "    dataset_used = 'filtered'\n",
    "    print(\"The arm swing quantification is based on the filtered gait segments.\\n\")\n",
    "else:\n",
    "    dataset_used = 'unfiltered'\n",
    "    print(\"The arm swing quantification is based on all gait segments.\\n\")\n",
    "\n",
    "quantified_arm_swing, gait_segment_meta = quantify_arm_swing(\n",
    "    df=df,\n",
    "    fs=config.sampling_frequency,\n",
    "    filtered=filtered,\n",
    "    max_segment_gap_s=config.max_segment_gap_s,\n",
    "    min_segment_length_s=config.min_segment_length_s,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Gait segments are created of minimum {config.min_segment_length_s} seconds \"\n",
    "    f\"and maximum {config.max_segment_gap_s} seconds gap between segments.\\n\"\n",
    ")\n",
    "print(\n",
    "    f\"A total of {quantified_arm_swing['gait_segment_nr'].nunique()} {dataset_used} \"\n",
    "    f\"gait segments have been quantified.\"\n",
    ")\n",
    "\n",
    "print(\"\\nMetadata of the first gait segment:\")\n",
    "print(json.dumps(gait_segment_meta['per_segment'][1], indent = 1))\n",
    "\n",
    "filt_example_s = gait_segment_meta['per_segment'][1]['duration_filtered_segment_s']\n",
    "unfilt_example_s = gait_segment_meta['per_segment'][1]['duration_unfiltered_segment_s']\n",
    "print(\n",
    "    f\"\\nOf this example, the filtered gait segment of {filt_example_s} seconds \"\n",
    "    f\"is part of an unfiltered segment of {unfilt_example_s} seconds, which is \"\n",
    "    f\"at least as large as the filtered gait segment.\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\nIndividual arm swings of the first gait segment of the \"\n",
    "    f\" {dataset_used} dataset:\"\n",
    ")\n",
    "quantified_arm_swing.loc[quantified_arm_swing['gait_segment_nr'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run steps 1-6 for the all raw data segment(s) <a id='multiple_segments_cell'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your data is also stored in multiple raw data segments, you can modify `raw_data_segments` in the cell below to a list of the filenames of your respective segmented data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to where the prepared data is saved\n",
    "path_to_data =  Path('../../example_data/verily')\n",
    "path_to_prepared_data = path_to_data / 'imu'\n",
    "\n",
    "# Load the gait detection classifier package\n",
    "classifier_package_filename = 'gait_detection_clf_package.pkl'\n",
    "full_path_to_classifier_package = (\n",
    "    files('paradigma')\n",
    "    / 'assets'\n",
    "    / classifier_package_filename\n",
    ")\n",
    "clf_package_detection = ClassifierPackage.load(full_path_to_classifier_package)\n",
    "\n",
    "# Load the gait filtering classifier package\n",
    "classifier_package_filename = 'gait_filtering_clf_package.pkl'\n",
    "full_path_to_classifier_package = (\n",
    "    files('paradigma')\n",
    "    / 'assets'\n",
    "    / classifier_package_filename\n",
    ")\n",
    "clf_package_filtering = ClassifierPackage.load(full_path_to_classifier_package)\n",
    "\n",
    "# Set to True to quantify arm swing based on the filtered gait segments, and\n",
    "# False to quantify arm swing based on all gait segments\n",
    "filtered = True\n",
    "\n",
    "# Create a list to store all quantified arm swing segments\n",
    "list_quantified_arm_swing = []\n",
    "max_gait_segment_nr = 0\n",
    "\n",
    "raw_data_segments  = ['0001']  # list with all available raw data segments\n",
    "\n",
    "for raw_data_segment_nr in raw_data_segments:\n",
    "\n",
    "    # Load the data\n",
    "    df_imu, _, _ = load_tsdf_dataframe(\n",
    "        path_to_prepared_data,\n",
    "        prefix=f'IMU_segment{raw_data_segment_nr}'\n",
    "    )\n",
    "\n",
    "    # 1: Preprocess the data\n",
    "    # Change column names if necessary by creating parameter column_mapping\n",
    "    # (see previous cells for an example)\n",
    "    config = IMUConfig()\n",
    "\n",
    "    df_preprocessed = preprocess_imu_data(\n",
    "        df=df_imu,\n",
    "        config=config,\n",
    "        sensor='both',\n",
    "        watch_side='left',\n",
    "    )\n",
    "\n",
    "    # 2: Extract gait features\n",
    "    config = GaitConfig(step='gait')\n",
    "\n",
    "    df_gait = extract_gait_features(\n",
    "        df=df_preprocessed,\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    # 3: Detect gait\n",
    "    df_gait[DataColumns.PRED_GAIT_PROBA] = detect_gait(\n",
    "        df=df_gait,\n",
    "        clf_package=clf_package_detection\n",
    "    )\n",
    "\n",
    "    # Merge gait predictions into timeseries data\n",
    "    if not any(\n",
    "        df_gait[DataColumns.PRED_GAIT_PROBA] >= clf_package_detection.threshold\n",
    "    ):\n",
    "        raise ValueError(\"No gait detected in the input data.\")\n",
    "\n",
    "    df = merge_predictions_with_timestamps(\n",
    "        df_ts=df_preprocessed,\n",
    "        df_predictions=df_gait,\n",
    "        pred_proba_colname=DataColumns.PRED_GAIT_PROBA,\n",
    "        window_length_s=config.window_length_s,\n",
    "        fs=config.sampling_frequency\n",
    "    )\n",
    "\n",
    "    df[DataColumns.PRED_GAIT] = (\n",
    "        df[DataColumns.PRED_GAIT_PROBA] >= clf_package_detection.threshold\n",
    "    ).astype(int)\n",
    "    df = df.loc[df[DataColumns.PRED_GAIT]==1].reset_index(drop=True)\n",
    "\n",
    "    # 4: Extract arm activity features\n",
    "    config = GaitConfig(step='arm_activity')\n",
    "\n",
    "    df_arm_activity = extract_arm_activity_features(\n",
    "        df=df,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    # 5: Filter gait\n",
    "    df_arm_activity[DataColumns.PRED_NO_OTHER_ARM_ACTIVITY_PROBA] = filter_gait(\n",
    "        df=df_arm_activity,\n",
    "        clf_package=clf_package_filtering\n",
    "    )\n",
    "\n",
    "    # Merge arm activity predictions into timeseries data\n",
    "    if not any(\n",
    "        df_arm_activity[DataColumns.PRED_NO_OTHER_ARM_ACTIVITY_PROBA] >= filt_threshold\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"No gait without other arm activities detected in the input data.\"\n",
    "        )\n",
    "\n",
    "    df = merge_predictions_with_timestamps(\n",
    "        df_ts=df_preprocessed,\n",
    "        df_predictions=df_arm_activity,\n",
    "        pred_proba_colname=DataColumns.PRED_NO_OTHER_ARM_ACTIVITY_PROBA,\n",
    "        window_length_s=config.window_length_s,\n",
    "        fs=config.sampling_frequency\n",
    "    )\n",
    "\n",
    "    df[DataColumns.PRED_NO_OTHER_ARM_ACTIVITY] = (\n",
    "        df[DataColumns.PRED_NO_OTHER_ARM_ACTIVITY_PROBA] >= filt_threshold\n",
    "    ).astype(int)\n",
    "    df = df.loc[df[DataColumns.PRED_NO_OTHER_ARM_ACTIVITY]==1].reset_index(drop=True)\n",
    "\n",
    "    # 6: Quantify arm swing\n",
    "    quantified_arm_swing, gait_segment_meta = quantify_arm_swing(\n",
    "        df=df,\n",
    "        fs=config.sampling_frequency,\n",
    "        filtered=filtered,\n",
    "        max_segment_gap_s=config.max_segment_gap_s,\n",
    "        min_segment_length_s=config.min_segment_length_s,\n",
    "    )\n",
    "\n",
    "    # Since gait segments start at zero, and we are concatenating multiple segments,\n",
    "    # we need to update the gait segment numbers to avoid aggregating multiple\n",
    "    # gait segments with the same number\n",
    "    if len(list_quantified_arm_swing) == 0:\n",
    "        max_gait_segment_nr = 0\n",
    "    else:\n",
    "        max_gait_segment_nr = quantified_arm_swing['gait_segment_nr'].max()\n",
    "\n",
    "    quantified_arm_swing['gait_segment_nr'] += max_gait_segment_nr\n",
    "    gait_segment_meta['per_segment'] = {\n",
    "        k + max_gait_segment_nr: v for k, v in gait_segment_meta['per_segment'].items()\n",
    "    }\n",
    "\n",
    "    # Add the predictions of the current raw data segment to the list\n",
    "    quantified_arm_swing['raw_data_segment_nr'] = raw_data_segment_nr\n",
    "    list_quantified_arm_swing.append(quantified_arm_swing)\n",
    "\n",
    "quantified_arm_swing = pd.concat(list_quantified_arm_swing, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Aggregation\n",
    "Finally, the arm swing estimates can be aggregated across all gait segments. \n",
    "\n",
    "Optionally, gait segments can be categorized into bins of specific length. Bins are tuples *(a, b)* including *a* and excluding *b*, i.e., gait segments `â‰¥ a` seconds and `< b` seconds. For example, to analyze gait segments of at least 20 seconds, the tuple `(20, np.inf)` can be used. In case you want to analyze all gait segments combined, use `(0, np.inf)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_categories = [(0,10), (10,20), (20, np.inf), (0, np.inf)]\n",
    "\n",
    "arm_swing_aggregations = aggregate_arm_swing_params(\n",
    "    df_arm_swing_params=quantified_arm_swing,\n",
    "    segment_meta=gait_segment_meta['per_segment'],\n",
    "    segment_cats=segment_categories,\n",
    "    aggregates=['median', '95p']\n",
    ")\n",
    "\n",
    "print(json.dumps(arm_swing_aggregations, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the aggregation step contains the aggregated arm swing parameters per gait segment category. Additionally, the total time in seconds `time_s` is added to inform based on how much data the aggregations were created."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
