{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# ParaDigMa Pipeline Orchestrator Tutorial\n",
    "\n",
    "This tutorial demonstrates how to use the **pipeline orchestrator** `run_paradigma()`, which serves as the main entry point for running ParaDigMa analysis pipelines. The orchestrator coordinates multiple analysis steps and can process different formats of sensor data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The `run_paradigma()` function is called an _orchestrator_ because it coordinates multiple analysis steps depending on the user input. It can process:\n",
    "\n",
    "- **Gait analysis**: Arm swing quantification from IMU data\n",
    "- **Tremor analysis**: Tremor detection and quantification from gyroscope data  \n",
    "- **Pulse rate estimation**: Pulse rate analysis from PPG data\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Multi-pipeline support**: Run multiple analyses simultaneously\n",
    "- **Flexible data input**: Works with both prepared and raw sensor data\n",
    "- **Multiple data formats**: Supports Verily, Axivity, Empatica, and custom formats\n",
    "- **Robust processing**: Automatic data preparation and error handling\n",
    "\n",
    "### Data Requirements\n",
    "\n",
    "The orchestrator accepts either:\n",
    "1. **Prepared data**: Prepared according to the [Data preparation tutorial](https://biomarkersparkinson.github.io/paradigma/tutorials/data_preparation.html)\n",
    "2. **Raw data**: Automatically processed (note: this feature has a limited scope)\n",
    "\n",
    "Let's explore different usage scenarios with examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Important required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from paradigma.constants import TimeUnit\n",
    "from paradigma.load import load_data_files\n",
    "from paradigma.orchestrator import run_paradigma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 1. Single pipeline with prepared data\n",
    "\n",
    "Let's start with a simple example using prepared PPG data for pulse rate analysis. \n",
    "\n",
    "The function `load_data_files` attempts to load data of any or multiple of the following formats: \n",
    "'parquet', 'csv', 'pkl', 'pickle', 'json', 'avro', 'cwa'. You can load the data in your preferred \n",
    "ways, but note that the output should be of format `Dict[str, pd.DataFrame]`:\n",
    "```python\n",
    "{\n",
    "    'file_1': df_1, \n",
    "    'file_2': df_2, \n",
    "    ..., \n",
    "    'file_n': df_n\n",
    "}\n",
    "```\n",
    "\n",
    "Alternatively, you can provide:\n",
    "- A **single DataFrame**: Will be processed with key `'segment_1'`\n",
    "- A **list of DataFrames**: Each will get keys like `'segment_1'`, `'segment_2'`, etc.\n",
    "\n",
    "This means ParaDigMa can run multiple files in sequence. This is useful when you have multiple files\n",
    "spanning a week, and you want aggregations to be computed across all files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_ppg_data = Path('../../example_data/verily/ppg')\n",
    "\n",
    "dfs_ppg = load_data_files(\n",
    "    data_path=path_to_ppg_data,\n",
    "    file_patterns='json',\n",
    "    verbosity=0 \n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(dfs_ppg)} PPG files:\")\n",
    "for filename in dfs_ppg.keys():\n",
    "    df = dfs_ppg[filename]\n",
    "    print(f\"  - {filename}: {len(df)} samples, {len(df.columns)} columns\")\n",
    "\n",
    "print(f\"\\nFirst 5 rows of {list(dfs_ppg.keys())[0]}:\")\n",
    "dfs_ppg[list(dfs_ppg.keys())[0]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Output Control\n",
    "\n",
    "When running ParaDigMa, you can control where results are saved and what intermediate results to store:\n",
    "\n",
    "**Output Directory:**\n",
    "- Default: `output_dir` defaults to `\"./output\"` \n",
    "- Custom: Specify your own path like `output_dir=\"./my_results\"`\n",
    "- No storage: Files are only saved if `store_intermediate` is not empty\n",
    "\n",
    "**Store Intermediate Results:**\n",
    "\n",
    "The `store_intermediate` parameter accepts a list of strings:\n",
    "```python\n",
    "store_intermediate=['preprocessing', 'quantification', 'aggregation']\n",
    "```\n",
    "\n",
    "Valid options are:\n",
    "- `'preparation'`: Save prepared data\n",
    "- `'preprocessing'`: Save preprocessed signals\n",
    "- `'classification'`: Save classification results\n",
    "- `'quantification'`: Save quantified measures\n",
    "- `'aggregation'`: Save aggregated results\n",
    "\n",
    "If `store_intermediate=[]` (empty list), **no files are saved** - results are only returned in memory.\n",
    "\n",
    "Also, set the correct units of the `time` column. For all options, please check [the API reference](https://biomarkersparkinson.github.io/paradigma/autoapi/paradigma/constants/index.html#paradigma.constants.TimeUnit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = 'pulse_rate'\n",
    "\n",
    "# Example 1: Using default output directory with storage\n",
    "results_single_pipeline = run_paradigma(\n",
    "    dfs=dfs_ppg,\n",
    "    pipeline_names=pipeline,\n",
    "    data_prepared=True,\n",
    "    time_input_unit=TimeUnit.RELATIVE_S,\n",
    "    store_intermediate=['quantification', 'aggregation'],  # Files saved to ./output\n",
    "    verbosity=1 \n",
    ")\n",
    "\n",
    "print(results_single_pipeline['metadata'][pipeline])\n",
    "print(results_single_pipeline['aggregations'][pipeline])\n",
    "results_single_pipeline['quantifications'][pipeline].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: No file storage - results only in memory\n",
    "results_no_storage = run_paradigma(\n",
    "    dfs=dfs_ppg,\n",
    "    pipeline_names=pipeline,\n",
    "    data_prepared=True,\n",
    "    time_input_unit=TimeUnit.RELATIVE_S,\n",
    "    store_intermediate=[],  # No files saved\n",
    "    verbosity=1 \n",
    ")\n",
    "\n",
    "print(\"Results returned without file storage:\")\n",
    "print(f\"  Quantifications: {len(results_no_storage['quantifications'][pipeline])} rows\")\n",
    "print(f\"  Aggregations: {results_no_storage['aggregations'][pipeline]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Example: No File Storage\n",
    "\n",
    "If you only want to work with results in memory without saving any files, use an empty `store_intermediate` list:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Note that `run_paradigma` currently does not accept accelerometer data as a supplement to the pulse\n",
    "rate pipeline for signal quality analysis. If you want to do these analyses, please check out the\n",
    "[Pulse rate analysis](https://biomarkersparkinson.github.io/paradigma/tutorials/_static/pulse_rate_analysis.html)\n",
    "tutorial for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 2. Multi-pipeline with prepared data\n",
    "\n",
    "One of the key features of the orchestrator is the ability to run multiple analysis pipelines simultaneously on the same data. This is more efficient than running them separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Results Structure\n",
    "\n",
    "The multi-pipeline orchestrator returns a nested structure that organizes results by pipeline:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'quantifications': {\n",
    "        'gait': DataFrame,      # Gait segment-level quantifications\n",
    "        'tremor': DataFrame     # Tremor window-level quantifications\n",
    "    },\n",
    "    'aggregations': {\n",
    "        'gait': {...},         # Aggregated gait metrics\n",
    "        'tremor': {...}        # Aggregated tremor metrics  \n",
    "    },\n",
    "    'metadata': {\n",
    "        'gait': {...},         # Gait analysis metadata\n",
    "        'tremor': {...}        # Tremor analysis metadata\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prepared IMU data\n",
    "path_to_imu_data = Path('../../example_data/verily/imu')\n",
    "\n",
    "dfs_imu = load_data_files(\n",
    "    data_path=path_to_imu_data,\n",
    "    file_patterns='json',\n",
    "    verbosity=0 \n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(dfs_imu)} IMU files:\")\n",
    "for filename in dfs_imu.keys():\n",
    "    df = dfs_imu[filename]\n",
    "    print(f\"  - {filename}: {len(df)} samples, {len(df.columns)} columns\")\n",
    "\n",
    "print(f\"\\nFirst 5 rows of {list(dfs_imu.keys())[0]}:\")\n",
    "dfs_imu[list(dfs_imu.keys())[0]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run gait and tremor analysis on the prepared data\n",
    "# Using custom output directory\n",
    "results_multi_pipeline = run_paradigma(\n",
    "    output_dir=Path('./output_multi'),\n",
    "    dfs=dfs_imu,                        # Pre-loaded data\n",
    "    data_prepared=True,                 # Data is already prepared\n",
    "    pipeline_names=['gait', 'tremor'],  # Multiple pipelines (list format)\n",
    "    watch_side='left',                  # Required for gait analysis\n",
    "    store_intermediate=['quantification'],  # Store quantifications only\n",
    "    verbosity=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_multi_pipeline.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the results structure\n",
    "print(\"Detailed Results Analysis:\")\n",
    "\n",
    "# Gait results\n",
    "arm_swing_quantified = results_multi_pipeline['quantifications']['gait']\n",
    "arm_swing_aggregates = results_multi_pipeline['aggregations']['gait']\n",
    "arm_swing_meta = results_multi_pipeline['metadata']['gait']\n",
    "print(f\"\\nArm swing quantification ({len(arm_swing_quantified)} windows):\")\n",
    "print(f\"   Columns: {list(arm_swing_quantified.columns[:5])}... ({len(arm_swing_quantified.columns)} total)\")\n",
    "print(f\"   Files: {arm_swing_quantified['file_key'].unique()}\")\n",
    "\n",
    "print(f\"\\nArm swing aggregation ({len(arm_swing_aggregates)} time ranges):\")\n",
    "print(f\"   Gait segment categories: {list(arm_swing_aggregates.keys())}\")\n",
    "print(f\"   Aggregates: {list(arm_swing_aggregates['0_10'].keys())}\")\n",
    "print(f\"   Metadata first gait segment: {arm_swing_meta[1]}\")\n",
    "\n",
    "# Tremor results  \n",
    "tremor_quantified = results_multi_pipeline['quantifications']['tremor']\n",
    "tremor_aggregates = results_multi_pipeline['aggregations']['tremor']\n",
    "tremor_meta = results_multi_pipeline['metadata']['tremor']\n",
    "print(f\"\\nTremor quantification ({len(tremor_quantified)} windows):\")\n",
    "print(f\"   Columns: {list(tremor_quantified.columns[:5])}... ({len(tremor_quantified.columns)} total)\")\n",
    "print(f\"   Files: {tremor_quantified['file_key'].unique()}\")\n",
    "\n",
    "print(f\"\\nTremor aggregation ({len(tremor_aggregates)} time ranges):\")\n",
    "print(f\"   Aggregates: {list(tremor_aggregates.keys())}\")\n",
    "print(f\"   Metadata first tremor segment: {tremor_meta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 3. Raw Data Processing\n",
    "\n",
    "The orchestrator can also process raw sensor data automatically. This includes data preparation steps like format standardization, unit conversion, and orientation correction. Note that this feature has been developed on limited data examples, and therefore may not function as expected on newly observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_raw_data = Path('../../example_data/axivity')\n",
    "\n",
    "device_orientation = [\"-x\", \"-y\", \"z\"]      # Sensor was worn upside-down\n",
    "pipeline = 'gait'\n",
    "\n",
    "# Working with raw data - this requires data preparation\n",
    "# Using custom output directory\n",
    "results_end_to_end = run_paradigma(\n",
    "    output_dir=Path('./output_raw'),\n",
    "    data_path=path_to_raw_data,             # Point to data folder\n",
    "    data_prepared=False,                    # ParaDigMa will prepare the data\n",
    "    pipeline_names=pipeline,\n",
    "    watch_side=\"left\",\n",
    "    time_input_unit=TimeUnit.RELATIVE_S,    # Specify time unit for raw data\n",
    "    accelerometer_units='g',\n",
    "    gyroscope_units='deg/s',\n",
    "    target_frequency=100.0,\n",
    "    device_orientation=device_orientation,\n",
    "    store_intermediate=['aggregation'],     # Only save aggregations\n",
    "    verbosity=1,\n",
    ")\n",
    "\n",
    "print(results_end_to_end['metadata'][pipeline][1])\n",
    "print(results_end_to_end['aggregations'][pipeline])\n",
    "results_end_to_end['quantifications'][pipeline].head()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
