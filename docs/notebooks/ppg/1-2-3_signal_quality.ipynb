{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main script to perform heart rate estimation of wearable PPG\n",
    "\n",
    "This script uses both PPG and accelerometer and performs the following steps:\n",
    "1. Loading all metadata of PPG and IMU\n",
    "2. Query on data availability + synchronization\n",
    "3. Loading relevant segment sensor data using tsdf wrapper (start for loop over synchronized segment indices)\n",
    "4. Synchronize the data (correct indices etc)\n",
    "5. Data preprocessing\n",
    "6. Feature extraction\n",
    "7. Classification\n",
    "\n",
    "\n",
    "## Architecture overview\n",
    "The script implements the following steps:\n",
    " - Step 1: IMU and PPG preprocessing\n",
    " - Step 2: IMU and PPG feature extraction\n",
    " - Step 3: Signal quality assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Automatically reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "import tsdf\n",
    "from dbpd.constants import DataColumns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "sensor_imu = 'IMU'\n",
    "sensor_ppg = 'PPG'\n",
    "\n",
    "imu_meta_filename = f'{sensor_imu}_meta.json'\n",
    "imu_values_filename = f'{sensor_imu}_samples.bin'\n",
    "imu_time_filename = f'{sensor_imu}_time.bin'\n",
    "\n",
    "ppg_meta_filename = f'{sensor_ppg}_meta.json'\n",
    "ppg_values_filename = f'{sensor_ppg}_samples.bin'\n",
    "ppg_time_filename = f'{sensor_ppg}_time.bin'\n",
    "\n",
    "rotation_units = 'deg/s'\n",
    "acceleration_units = 'm/s^2'\n",
    "\n",
    "d_channels_units = {\n",
    "    DataColumns.ACCELEROMETER_X: acceleration_units,\n",
    "    DataColumns.ACCELEROMETER_Y: acceleration_units,\n",
    "    DataColumns.ACCELEROMETER_Z: acceleration_units,\n",
    "    DataColumns.GYROSCOPE_X: rotation_units,\n",
    "    DataColumns.GYROSCOPE_Y: rotation_units,\n",
    "    DataColumns.GYROSCOPE_Z: rotation_units,\n",
    "\n",
    "}\n",
    "\n",
    "# filtering\n",
    "sampling_frequency = 100\n",
    "lower_cutoff_frequency = 0.3\n",
    "filter_order = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module methods\n",
    "\n",
    "def tsdf_scan_meta(tsdf_data_full_path : str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    For each given TSDF directory, transcribe TSDF metadata contents to a list of dictionaries.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tsdf_data_full_path : str\n",
    "        Full path to the directory containing TSDF metadata files.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[Dict]\n",
    "        List of dictionaries with metadata from each JSON file in the directory.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> tsdf_scan_meta('/path/to/tsdf_data')\n",
    "    [{'start_iso8601': '2021-06-27T16:52:20Z', 'end_iso8601': '2021-06-27T17:52:20Z'}, ...]\n",
    "    \"\"\"\n",
    "    tsdf = []\n",
    "    \n",
    "    # Collect all metadata JSON files in the specified directory\n",
    "    meta_list = list(Path(tsdf_data_full_path).rglob('*_meta.json'))\n",
    "    for meta_file in meta_list:\n",
    "        with open(meta_file, 'r') as file:\n",
    "            json_obj = json.load(file)\n",
    "            meta_data = {\n",
    "                'tsdf_meta_fullpath': str(meta_file),\n",
    "                'subject_id': json_obj['subject_id'],\n",
    "                'start_iso8601': json_obj['start_iso8601'],\n",
    "                'end_iso8601': json_obj['end_iso8601']\n",
    "            }\n",
    "            tsdf.append(meta_data)\n",
    "    \n",
    "    return tsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "UNIX_TICKS_MS = 1000.0\n",
    "FS_PPG = 30  # Sampling rate for PPG\n",
    "FS_IMU = 100  # Sampling rate for IMU\n",
    "\n",
    "# Paths\n",
    "raw_data_root = '../../../tests/data/1.sensor_data/'\n",
    "ppp_data_path_ppg = os.path.join(raw_data_root, 'PPG')\n",
    "ppp_data_path_imu = os.path.join(raw_data_root, 'IMU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Loading all metadata of PPG and IMU\n",
    "\n",
    "meta_ppg = tsdf_scan_meta(ppp_data_path_ppg)\n",
    "meta_imu = tsdf_scan_meta(ppp_data_path_imu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "\n",
    "def convert_iso8601_to_datetime(date_str):\n",
    "        \"\"\"\n",
    "        Convert a date string to a datetime object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        date_str : str\n",
    "            Date string in the format '%d-%b-%Y %H:%M:%S %Z'.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        datetime\n",
    "            A datetime object corresponding to the input date string.\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> convert_to_datetime('27-Jun-2021 16:52:20 UTC')\n",
    "        datetime.datetime(2021, 6, 27, 16, 52, 20, tzinfo=<UTC>)\n",
    "        \"\"\"\n",
    "        return datetime.strptime(date_str, '%d-%b-%Y %H:%M:%S %Z')\n",
    "\n",
    "def synchronization(ppg_meta, imu_meta):\n",
    "    \"\"\"\n",
    "    Synchronize PPG and IMU data segments based on their start and end times.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ppg_meta : list of dict\n",
    "        List of dictionaries containing 'start_iso8601' and 'end_iso8601' keys for PPG data.\n",
    "    imu_meta : list of dict\n",
    "        List of dictionaries containing 'start_iso8601' and 'end_iso8601' keys for IMU data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    segment_ppg_total : list of int\n",
    "        List of synchronized segment indices for PPG data.\n",
    "    segment_imu_total : list of int\n",
    "        List of synchronized segment indices for IMU data.\n",
    "    \"\"\"\n",
    "    ppg_start_time = [convert_iso8601_to_datetime(t['start_iso8601']) for t in ppg_meta]\n",
    "    imu_start_time = [convert_iso8601_to_datetime(t['start_iso8601']) for t in imu_meta]\n",
    "    ppg_end_time = [convert_iso8601_to_datetime(t['end_iso8601']) for t in ppg_meta]\n",
    "    imu_end_time = [convert_iso8601_to_datetime(t['end_iso8601']) for t in imu_meta]\n",
    "\n",
    "    # Create a time vector covering the entire range\n",
    "    time_vector_total = []\n",
    "    current_time = min(min(ppg_start_time), min(imu_start_time))\n",
    "    end_time = max(max(ppg_end_time), max(imu_end_time))\n",
    "    while current_time <= end_time:\n",
    "        time_vector_total.append(current_time)\n",
    "        current_time += timedelta(seconds=1)\n",
    "    \n",
    "    time_vector_total = np.array(time_vector_total)\n",
    "\n",
    "    # Initialize variables\n",
    "    data_presence_ppg = np.zeros(len(time_vector_total), dtype=int)\n",
    "    data_presence_ppg_idx = np.zeros(len(time_vector_total), dtype=int)\n",
    "    data_presence_imu = np.zeros(len(time_vector_total), dtype=int)\n",
    "    data_presence_imu_idx = np.zeros(len(time_vector_total), dtype=int)\n",
    "\n",
    "    # Mark the segments of PPG data with 1\n",
    "    for i, (start, end) in enumerate(zip(ppg_start_time, ppg_end_time)):\n",
    "        indices = np.where((time_vector_total >= start) & (time_vector_total < end))[0]\n",
    "        data_presence_ppg[indices] = 1\n",
    "        data_presence_ppg_idx[indices] = i\n",
    "\n",
    "    # Mark the segments of IMU data with 1\n",
    "    for i, (start, end) in enumerate(zip(imu_start_time, imu_end_time)):\n",
    "        indices = np.where((time_vector_total >= start) & (time_vector_total < end))[0]\n",
    "        data_presence_imu[indices] = 1\n",
    "        data_presence_imu_idx[indices] = i\n",
    "\n",
    "    # Find the indices where both PPG and IMU data are present\n",
    "    corr_indices = np.where((data_presence_ppg == 1) & (data_presence_imu == 1))[0]\n",
    "\n",
    "    # Find the start and end indices of each segment\n",
    "    corr_start_end = []\n",
    "    if len(corr_indices) > 0:\n",
    "        start_idx = corr_indices[0]\n",
    "        for i in range(1, len(corr_indices)):\n",
    "            if corr_indices[i] - corr_indices[i - 1] > 1:\n",
    "                end_idx = corr_indices[i - 1]\n",
    "                corr_start_end.append((start_idx, end_idx))\n",
    "                start_idx = corr_indices[i]\n",
    "        # Add the last segment\n",
    "        corr_start_end.append((start_idx, corr_indices[-1]))\n",
    "\n",
    "    # Extract the synchronized indices for each segment\n",
    "    segment_ppg_total = []\n",
    "    segment_imu_total = []\n",
    "    for start_idx, end_idx in corr_start_end:\n",
    "        segment_ppg = np.unique(data_presence_ppg_idx[start_idx:end_idx + 1])\n",
    "        segment_imu = np.unique(data_presence_imu_idx[start_idx:end_idx + 1])\n",
    "        if len(segment_ppg) > 1 and len(segment_imu) == 1:\n",
    "            segment_ppg_total.extend(segment_ppg)\n",
    "            segment_imu_total.extend([segment_imu[0]] * len(segment_ppg))\n",
    "        elif len(segment_ppg) == 1 and len(segment_imu) > 1:\n",
    "            segment_ppg_total.extend([segment_ppg[0]] * len(segment_imu))\n",
    "            segment_imu_total.extend(segment_imu)\n",
    "        elif len(segment_ppg) == len(segment_imu):\n",
    "            segment_ppg_total.extend(segment_ppg)\n",
    "            segment_imu_total.extend(segment_imu)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return segment_ppg_total, segment_imu_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Query on data availability + synchronization\n",
    "segment_ppg, segment_imu = synchronization(meta_ppg, meta_imu)  # Define `synchronization`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tsdf_values_idx(metadata_list: List[dict], suffix: str) -> List[int]:\n",
    "    \"\"\"\n",
    "    This does not work.\n",
    "    Searches for indices in the metadata list where the file name ends with a specified suffix. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    metadata_list : list of dict\n",
    "        A list where each item is a dictionary containing metadata, including a 'file_name' key with the file's name as its value.\n",
    "    suffix : str\n",
    "        The suffix to search for in the file names. It should include the extension, such as '.bin'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of int\n",
    "        A list of indices where the file names end with the specified suffix.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> metadata_list = [{'file_name': 'data1.bin'}, {'file_name': 'data2.txt'}, {'file_name': 'data3.bin'}]\n",
    "    >>> tsdf_values_idx(metadata_list, '.bin')\n",
    "    [0, 2]\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    pattern = re.compile(rf\".*{re.escape(suffix)}$\")\n",
    "    \n",
    "    for i, metadata in enumerate(metadata_list):\n",
    "        # Ensure the metadata is a dictionary and contains the 'file_name' key\n",
    "        if isinstance(metadata, dict) and 'file_name' in metadata:\n",
    "            # Check if the 'file_name' matches the pattern\n",
    "            if pattern.search(metadata['file_name']):\n",
    "                indices.append(i)\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_list_ppg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m ts_imu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(datetime_imu\u001b[38;5;241m.\u001b[39mtimestamp() \u001b[38;5;241m*\u001b[39m UNIX_TICKS_MS)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Calculating continuous time vectors\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m t_ppg \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcumsum(\u001b[43mdata_list_ppg\u001b[49m[time_idx_ppg]) \u001b[38;5;241m+\u001b[39m ts_ppg\n\u001b[1;32m     24\u001b[0m t_imu \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcumsum(data_list_imu[time_idx_imu]) \u001b[38;5;241m+\u001b[39m ts_imu\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_list_ppg' is not defined"
     ]
    }
   ],
   "source": [
    "# 3. Loading relevant segment sensor data\n",
    "n = 0  # Assuming we're only looking at the first synchronized segment\n",
    "meta_path_ppg = meta_ppg[segment_ppg[n]]['tsdf_meta_fullpath']\n",
    "meta_path_imu = meta_imu[segment_imu[n]]['tsdf_meta_fullpath']\n",
    "\n",
    "metadata_list_ppg = tsdf.load_metadata_from_path(meta_path_ppg)\n",
    "metadata_list_imu = tsdf.load_metadata_from_path(meta_path_imu)\n",
    "\n",
    "# Extract indices for time and samples (TSDF should support extracting data based on the channel names)\n",
    "time_idx_ppg = metadata_list_ppg[\"PPG_time.bin\"]\n",
    "time_idx_imu = metadata_list_imu[\"IMU_time.bin\"]\n",
    "values_idx_ppg = metadata_list_ppg[\"PPG_samples.bin\"]\n",
    "values_idx_imu = metadata_list_imu[\"IMU_samples.bin\"]\n",
    "\n",
    "# Process time data\n",
    "datetime_ppg = datetime.strptime(time_idx_ppg.start_iso8601, '%d-%b-%Y %H:%M:%S %Z')\n",
    "datetime_imu = datetime.strptime(time_idx_imu.start_iso8601, '%d-%b-%Y %H:%M:%S %Z')\n",
    "\n",
    "ts_ppg = int(datetime_ppg.timestamp() * UNIX_TICKS_MS)\n",
    "ts_imu = int(datetime_imu.timestamp() * UNIX_TICKS_MS)\n",
    "\n",
    "# Calculating continuous time vectors\n",
    "t_ppg = np.cumsum(data_list_ppg[time_idx_ppg]) + ts_ppg\n",
    "t_imu = np.cumsum(data_list_imu[time_idx_imu]) + ts_imu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Data synchronization on right indices\n",
    "ppg_indices, imu_indices = extract_overlapping_segments(t_ppg, t_imu)  # Define this function\n",
    "\n",
    "# Update data vectors based on synchronized indices\n",
    "v_ppg = data_list_ppg[values_idx_ppg][ppg_indices[0]:ppg_indices[1]]\n",
    "v_imu = data_list_imu[values_idx_imu][imu_indices[0]:imu_indices[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'synchronization' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m meta_imu \u001b[38;5;241m=\u001b[39m tsdf_scan_meta(ppp_data_path_imu)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 2. Query on data availability + synchronization\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m segment_ppg, segment_imu \u001b[38;5;241m=\u001b[39m \u001b[43msynchronization\u001b[49m(meta_ppg, meta_imu)  \u001b[38;5;66;03m# Define `synchronization`\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 3. Loading relevant segment sensor data\u001b[39;00m\n\u001b[1;32m      9\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Assuming we're only looking at the first synchronized segment\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'synchronization' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. Data preprocessing\n",
    "# Implement `preprocessing_ppg` and `preprocessing_imu` to suit your data format\n",
    "v_ppg_pre, tr_ppg_pre = preprocessing_ppg(v_ppg, FS_PPG)\n",
    "v_imu_pre, tr_imu_pre = preprocessing_imu(v_imu, FS_IMU)\n",
    "\n",
    "# Save preprocessed data\n",
    "location = \"../../tests/data/2.preprocessed_data/ppg\"\n",
    "save_preprocessed_data(v_ppg_pre, tr_ppg_pre, v_imu_pre, tr_imu_pre, location)\n",
    "\n",
    "# Feature extraction and Classification\n",
    "# Assume feature extraction and classification functions are implemented\n",
    "features_ppg, features_imu = extract_features(v_ppg_pre, v_imu_pre)\n",
    "classification_results = classify_signals(features_ppg, features_imu)\n",
    "\n",
    "# Save the classification results\n",
    "save_classification_data(classification_results, location)\n",
    "\n",
    "# We need to implement:\n",
    "# - synchronization: to find overlapping segments between PPG and IMU data based on metadata\n",
    "# - extract_overlapping_segments: to calculate the correct indices for synchronized data segments\n",
    "# - preprocessing_ppg, preprocessing_imu: functions to preprocess the raw PPG and IMU data\n",
    "# - extract_features: to extract relevant features from the preprocessed data\n",
    "# - classify_signals: to perform the classification on the extracted features\n",
    "# - save_preprocessed_data, save_classification_data: functions to save data to files in a suitable format\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
