{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main script to perform heart rate estimation of wearable PPG\n",
    "\n",
    "This script uses both PPG and accelerometer and performs the following steps:\n",
    "1. Loading all metadata of PPG and IMU\n",
    "2. Query on data availability + synchronization\n",
    "3. Loading relevant segment sensor data using tsdf wrapper (start for loop over synchronized segment indices)\n",
    "4. Synchronize the data (correct indices etc)\n",
    "5. Data preprocessing\n",
    "6. Perform pseudo-smoothed Wigner-Ville distribution (PSWVD) on PPG\n",
    "7. Saving the HR estimates in tsdf format\n",
    "\n",
    "## Architecture overview\n",
    "The script implements the following steps:\n",
    " - Step 1: PPG preprocessing\n",
    " - Step 4: HR estimation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.signal import butter, filtfilt\n",
    "from pathlib import Path\n",
    "\n",
    "# Add paths to the toolbox and wrapper\n",
    "import sys\n",
    "sys.path.append('../../../../dbpd-toolbox')\n",
    "sys.path.append('../../../../tsdf4matlab')\n",
    "\n",
    "import tsdf\n",
    "import dbpd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tsdf_scan_meta(tsdf_data_full_path):\n",
    "    \"\"\"\n",
    "    For each given TSDF directory, transcribe TSDF metadata contents to a list of dictionaries.\n",
    "    This function is specific for a toolbox data structure mimicking the given MATLAB code.\n",
    "    \n",
    "    Parameters:\n",
    "    - tsdf_data_full_path: Full path to the directory containing TSDF metadata files.\n",
    "    \n",
    "    Returns:\n",
    "    - tsdf: List of dictionaries with metadata from each JSON file in the directory.\n",
    "    \"\"\"\n",
    "    tsdf = []\n",
    "    \n",
    "    # Collect all metadata JSON files in the specified directory\n",
    "    meta_list = list(Path(tsdf_data_full_path).rglob('*_meta.json'))\n",
    "    for meta_file in meta_list:\n",
    "        with open(meta_file, 'r') as file:\n",
    "            json_obj = json.load(file)\n",
    "            meta_data = {\n",
    "                'tsdf_meta_fullpath': str(meta_file),\n",
    "                'subject_id': json_obj['subject_id'],\n",
    "                'start_iso8601': json_obj['start_iso8601'],\n",
    "                'end_iso8601': json_obj['end_iso8601']\n",
    "            }\n",
    "            tsdf.append(meta_data)\n",
    "    \n",
    "    return tsdf\n",
    "\n",
    "def tsdf_values_idx(metadata_list: list, suffix: str):\n",
    "    \"\"\"\n",
    "    Searches for indices in the metadata list where the file name ends with a specified suffix.\n",
    "    \n",
    "    Args:\n",
    "    - metadata_list (list of names): A list where each item is a dictionary containing metadata,\n",
    "      including a 'file_name' key with the file's name as its value.\n",
    "    - suffix (str): The suffix to search for in the file names. It should include the extension, \n",
    "      such as '.bin'.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of file names where the file names end with the specified suffix.\n",
    "    \"\"\"\n",
    "    # Compile a regular expression pattern to match file names ending with the given suffix\n",
    "    pattern = re.compile(rf\".*{re.escape(suffix)}$\")\n",
    "    for metadata in metadata_list:\n",
    "        # Check if the 'file_name' in metadata matches the pattern\n",
    "        if pattern.search(metadata):\n",
    "            return metadata\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4)\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "UNIX_TICKS_MS = 1000.0\n",
    "FS_PPG = 30\n",
    "\n",
    "# Paths\n",
    "raw_data_root = '../../../tests/data/1.sensor_data/'\n",
    "ppp_data_path_ppg = os.path.join(raw_data_root, 'PPG')\n",
    "meta_ppg = tsdf_scan_meta(ppp_data_path_ppg)\n",
    "\n",
    "sqa_data_path = '../../../tests/data/4.predictions/ppg'\n",
    "sqa_output_list = list(Path(sqa_data_path).glob('*_meta.json'))\n",
    "meta_path_sqa = sqa_output_list[0]\n",
    "\n",
    "\n",
    "# Load PPG data\n",
    "metadata_dict = tsdf.load_metadata_from_path(meta_path_sqa)\n",
    "\n",
    "metadata_binaries_list = metadata_dict.keys()\n",
    "\n",
    "# sync_idx = tsdf_values_idx(metadata_binaries_list, 'sync')\n",
    "sync_idx = \"classification_sqa_sync.bin\"\n",
    "data_sync_np:np.ndarray = tsdf.load_ndarray_from_binary(metadata_dict[sync_idx])\n",
    "\n",
    "\n",
    "# TODO: Fix the following code to work\n",
    "data_sync_np = data_sync_np[~(data_sync_np == 0).all(axis=1)]\n",
    "n_segments_sync = data_sync_np.shape[0]\n",
    "\n",
    "# Load classification data\n",
    "# ppg_prob_idx = tsdf_values_idx(metadata_binaries_list, 'ppg')\n",
    "ppg_prob_idx = \"classification_sqa_ppg.bin\"\n",
    "ppg_post_prob_np:np.ndarray = tsdf.load_ndarray_from_binary(metadata_dict[ppg_prob_idx])\n",
    "# imu_idx = tsdf_values_idx(metadata_binaries_list, 'sqa_imu')\n",
    "imu_idx = \"classification_sqa_imu.bin\"\n",
    "imu_label_np:np.ndarray = tsdf.load_ndarray_from_binary(metadata_dict[imu_idx])\n",
    "\n",
    "# Initialize the indices array for classification epochs\n",
    "print(data_sync_np.shape)\n",
    "start_end_indices = np.zeros((len(data_sync_np[:, 3]), 2), dtype=int)\n",
    "for i in range(len(data_sync_np[:, 3])):\n",
    "    if i == 0:\n",
    "        start_end_indices[i, 0] = 1\n",
    "    else:\n",
    "        start_end_indices[i, 0] = start_end_indices[i-1, 1] + 1\n",
    "    start_end_indices[i, 1] = np.sum(data_sync_np[:i+1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heart Rate Estimation Parameters\n",
    "MIN_WINDOW_LENGTH = 10\n",
    "MIN_HR_SAMPLES = MIN_WINDOW_LENGTH * FS_PPG\n",
    "THRESHOLD_SQA = 0.5\n",
    "HR_EST_LENGTH = 2\n",
    "HR_EST_SAMPLES = HR_EST_LENGTH * FS_PPG\n",
    "\n",
    "# Time-frequency Distribution Parameters\n",
    "TFD_LENGTH = 10\n",
    "KERN_TYPE = 'sep'  # Placeholder type\n",
    "WIN_TYPE_DOPPLER = 'hamm'\n",
    "WIN_TYPE_LAG = 'hamm'\n",
    "WIN_LENGTH_DOPPLER = 1\n",
    "WIN_LENGTH_LAG = 8\n",
    "DOPPLER_SAMPLES = FS_PPG * WIN_LENGTH_DOPPLER\n",
    "LAG_SAMPLES = WIN_LENGTH_LAG * FS_PPG\n",
    "KERN_PARAMS = {'doppler_samples': DOPPLER_SAMPLES, 'win_type_doppler': WIN_TYPE_DOPPLER, 'lag_samples': LAG_SAMPLES, 'win_type_lag': WIN_TYPE_LAG}\n",
    "\n",
    "# Placeholder for moving average filter\n",
    "MA = {\n",
    "    'value': 1,\n",
    "    'window': 30,\n",
    "    'FC': np.ones(30) / 30  # Filter coefficients\n",
    "}\n",
    "\n",
    "v_hr_ppg = []\n",
    "t_hr_unix = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m class_start \u001b[38;5;241m=\u001b[39m start_end_indices[n, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      7\u001b[0m class_end \u001b[38;5;241m=\u001b[39m start_end_indices[n, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m----> 9\u001b[0m meta_path_ppg \u001b[38;5;241m=\u001b[39m \u001b[43mmeta_ppg\u001b[49m\u001b[43m[\u001b[49m\u001b[43mppg_segment\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtsdf_meta_fullpath\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# TODO: Fix loading tsdf\u001b[39;00m\n\u001b[1;32m     11\u001b[0m metadata_list_ppg, data_list_ppg \u001b[38;5;241m=\u001b[39m tsdf\u001b[38;5;241m.\u001b[39mload_tsdf_metadata_from_path(meta_path_ppg)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Main Loop over all synchronized segments\n",
    "for n in range(n_segments_sync):\n",
    "    ppg_indices = data_sync_np[n, :2]\n",
    "    ppg_segment = data_sync_np[n, 2]\n",
    "\n",
    "    class_start = start_end_indices[n, 0]\n",
    "    class_end = start_end_indices[n, 1]\n",
    "\n",
    "    meta_path_ppg = meta_ppg[ppg_segment]['tsdf_meta_fullpath']\n",
    "    # TODO: Fix loading tsdf\n",
    "    metadata_list_ppg, data_list_ppg = tsdf.load_tsdf_metadata_from_path(meta_path_ppg)\n",
    "\n",
    "    time_idx_ppg = tsdf.get_index(metadata_list_ppg, 'time')\n",
    "    values_idx_ppg = tsdf.get_index(metadata_list_ppg, 'samples')\n",
    "\n",
    "    t_iso_ppg = metadata_list_ppg[time_idx_ppg]['start_iso8601']\n",
    "    datetime_ppg = datetime.strptime(t_iso_ppg, '%d-%b-%Y %H:%M:%S %Z').replace(tzinfo=datetime.timezone.utc)\n",
    "    ts_ppg = datetime_ppg.timestamp() * UNIX_TICKS_MS\n",
    "\n",
    "    t_ppg = np.cumsum(data_list_ppg[time_idx_ppg]) + ts_ppg\n",
    "    tr_ppg = (t_ppg - ts_ppg) / UNIX_TICKS_MS\n",
    "\n",
    "    v_ppg = data_list_ppg[values_idx_ppg]\n",
    "\n",
    "    v_ppg = v_ppg[ppg_indices[0]:ppg_indices[1]]\n",
    "    tr_ppg = tr_ppg[ppg_indices[0]:ppg_indices[1]]\n",
    "\n",
    "    ts_sync = ts_ppg + tr_ppg[0] * UNIX_TICKS_MS\n",
    "    tr_ppg -= tr_ppg[0]\n",
    "\n",
    "    fs_ppg_est = 1 / np.median(np.diff(tr_ppg))\n",
    "\n",
    "    if len(v_ppg) < FS_PPG * MIN_WINDOW_LENGTH:\n",
    "        print('Sample is of insufficient length!')\n",
    "        continue\n",
    "\n",
    "    # Placeholder functions\n",
    "    v_ppg_pre, tr_ppg_pre = preprocessing_ppg(tr_ppg, v_ppg, FS_PPG)\n",
    "    \n",
    "    class_ppg_segment = ppg_post_prob_np[class_start:class_end]\n",
    "    class_acc_segment = imu_label_np[class_start:class_end]\n",
    "\n",
    "    # Assign window-level probabilities to individual samples\n",
    "    data_prob_sample = sample_prob_final(class_ppg_segment, class_acc_segment, FS_PPG)\n",
    "\n",
    "    sqa_label = np.zeros(len(data_prob_sample))\n",
    "    sqa_label[data_prob_sample > THRESHOLD_SQA] = 1\n",
    "\n",
    "    v_start_idx, v_end_idx = extract_hr_segments(sqa_label, MIN_HR_SAMPLES)\n",
    "\n",
    "    for i in range(len(v_start_idx)):\n",
    "        if v_start_idx[i] < 2 * FS_PPG or v_end_idx[i] > len(v_ppg_pre) - 2 * FS_PPG:\n",
    "            continue\n",
    "        \n",
    "        rel_ppg = v_ppg_pre[v_start_idx[i]:v_end_idx[i]]\n",
    "        rel_time = tr_ppg_pre[v_start_idx[i]:v_end_idx[i]]\n",
    "\n",
    "        rel_ppg_spwvd = v_ppg_pre[v_start_idx[i] - FS_PPG*2:v_end_idx[i] + FS_PPG*2]\n",
    "        hr_est = PPG_TFD_HR(rel_ppg_spwvd, TFD_LENGTH, MA, FS_PPG, KERN_TYPE, KERN_PARAMS)\n",
    "\n",
    "        if len(rel_ppg) % 60 != 0:\n",
    "            hr_time = rel_time[0:len(rel_ppg) - FS_PPG:HR_EST_SAMPLES]\n",
    "        else:\n",
    "            hr_time = rel_time[0:len(rel_ppg):HR_EST_SAMPLES]\n",
    "\n",
    "        t_epoch_unix = hr_time * UNIX_TICKS_MS + ts_sync\n",
    "        v_hr_ppg.append(hr_est)\n",
    "        t_hr_unix.append(t_epoch_unix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the HR output in TSDF format\n",
    "data_hr_est = [np.array(t_hr_unix) / UNIX_TICKS_MS, np.array(v_hr_ppg)]\n",
    "\n",
    "location = \"../../../tests/data/5.quantification/ppg\"\n",
    "os.makedirs(location, exist_ok=True)\n",
    "\n",
    "if not t_hr_unix:\n",
    "    start_time_iso = datetime.fromtimestamp(0, tz=datetime.timezone.utc).strftime('%d-%b-%Y %H:%M:%S %Z')\n",
    "    end_time_iso = datetime.fromtimestamp(0, tz=datetime.timezone.utc).strftime('%d-%b-%Y %H:%M:%S %Z')\n",
    "else:\n",
    "    start_time_iso = datetime.fromtimestamp(t_hr_unix[0] / UNIX_TICKS_MS, tz=datetime.timezone.utc).strftime('%d-%b-%Y %H:%M:%S %Z')\n",
    "    end_time_iso = datetime.fromtimestamp(t_hr_unix[-1] / UNIX_TICKS_MS, tz=datetime.timezone.utc).strftime('%d-%b-%Y %H:%M:%S %Z')\n",
    "\n",
    "metafile_pre_template = metadata_binaries_list[time_idx_ppg]\n",
    "metafile_pre_template['start_iso8601'] = start_time_iso\n",
    "metafile_pre_template['end_iso8601'] = end_time_iso\n",
    "\n",
    "metafile_time = metafile_pre_template.copy()\n",
    "metafile_values_hr = metafile_pre_template.copy()\n",
    "\n",
    "metafile_time['channels'] = ['time']\n",
    "metafile_time['units'] = ['time_absolute_unix_s']\n",
    "metafile_time['file_name'] = 'hr_est_time.bin'\n",
    "\n",
    "metafile_values_hr['channels'] = ['HR estimates']\n",
    "metafile_values_hr['units'] = ['min^-1']\n",
    "metafile_values_hr['freq_sampling_original'] = round(fs_ppg_est, 2)\n",
    "metafile_values_hr['file_name'] = 'hr_est_values.bin'\n",
    "\n",
    "meta_class = [metafile_time, metafile_values_hr]\n",
    "\n",
    "mat_metadata_file_name = \"hr_est_meta.json\"\n",
    "tsdf.save_tsdf_data(meta_class, data_hr_est, location, mat_metadata_file_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
