{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main script to perform heart rate estimation of wearable PPG\n",
    "\n",
    "This script uses both PPG and accelerometer and performs the following steps:\n",
    "1. Loading all metadata of PPG and IMU\n",
    "2. Query on data availability + synchronization\n",
    "3. Loading relevant segment sensor data using tsdf wrapper (start for loop over synchronized segment indices)\n",
    "4. Synchronize the data (correct indices etc)\n",
    "5. Data preprocessing\n",
    "6. Perform pseudo-smoothed Wigner-Ville distribution (PSWVD) on PPG\n",
    "7. Saving the HR estimates in tsdf format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Automatically reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.signal import butter, filtfilt\n",
    "from pathlib import Path\n",
    "\n",
    "# Add paths to the toolbox and wrapper\n",
    "import sys\n",
    "sys.path.append('../../../../dbpd-toolbox')\n",
    "sys.path.append('../../../../tsdf4matlab')\n",
    "\n",
    "import tsdf\n",
    "import dbpd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsdf_scan_meta(tsdf_data_full_path):\n",
    "    \"\"\"\n",
    "    For each given TSDF directory, transcribe TSDF metadata contents to a list of dictionaries.\n",
    "    This function is specific for a toolbox data structure mimicking the given MATLAB code.\n",
    "    \n",
    "    Parameters:\n",
    "    - tsdf_data_full_path: Full path to the directory containing TSDF metadata files.\n",
    "    \n",
    "    Returns:\n",
    "    - tsdf: List of dictionaries with metadata from each JSON file in the directory.\n",
    "    \"\"\"\n",
    "    tsdf = []\n",
    "    \n",
    "    # Collect all metadata JSON files in the specified directory\n",
    "    meta_list = list(Path(tsdf_data_full_path).rglob('*_meta.json'))\n",
    "    for meta_file in meta_list:\n",
    "        with open(meta_file, 'r') as file:\n",
    "            json_obj = json.load(file)\n",
    "            meta_data = {\n",
    "                'tsdf_meta_fullpath': str(meta_file),\n",
    "                'subject_id': json_obj['subject_id'],\n",
    "                'start_iso8601': json_obj['start_iso8601'],\n",
    "                'end_iso8601': json_obj['end_iso8601']\n",
    "            }\n",
    "            tsdf.append(meta_data)\n",
    "    \n",
    "    return tsdf\n",
    "\n",
    "def tsdf_values_idx(metadata_list, suffix):\n",
    "    \"\"\"\n",
    "    Searches for indices in the metadata list where the file name ends with a specified suffix.\n",
    "    \n",
    "    Args:\n",
    "    - metadata_list (list of dicts): A list where each item is a dictionary containing metadata,\n",
    "      including a 'file_name' key with the file's name as its value.\n",
    "    - suffix (str): The suffix to search for in the file names. It should include the extension, \n",
    "      such as '.bin'.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of indices where the file names end with the specified suffix.\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    # Compile a regular expression pattern to match file names ending with the given suffix\n",
    "    pattern = re.compile(rf\".*{re.escape(suffix)}$\")\n",
    "    for i, metadata in enumerate(metadata_list):\n",
    "        # Check if the 'file_name' in metadata matches the pattern\n",
    "        if 'file_name' in metadata and pattern.search(metadata['file_name']):\n",
    "            indices.append(i)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m sqa_output_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(Path(sqa_data_path)\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*_meta.json\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     15\u001b[0m meta_path_sqa \u001b[38;5;241m=\u001b[39m sqa_output_list[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 17\u001b[0m metadata_list_sqa, data_list_sqa \u001b[38;5;241m=\u001b[39m tsdf\u001b[38;5;241m.\u001b[39mload_metadata_from_path(meta_path_sqa)\n\u001b[1;32m     18\u001b[0m sync_idx \u001b[38;5;241m=\u001b[39m tsdf_values_idx(metadata_list_sqa, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msync\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m data_sync \u001b[38;5;241m=\u001b[39m data_list_sqa[sync_idx]\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "np.seterr(all='ignore')  # Ignore warnings to improve speed\n",
    "\n",
    "# Constants\n",
    "UNIX_TICKS_MS = 1000.0\n",
    "FS_PPG = 30\n",
    "\n",
    "# Paths\n",
    "raw_data_root = '../../../tests/data/1.sensor_data/'\n",
    "ppp_data_path_ppg = os.path.join(raw_data_root, 'PPG')\n",
    "meta_ppg = tsdf_scan_meta(ppp_data_path_ppg)\n",
    "\n",
    "sqa_data_path = '../../../tests/data/4.predictions/ppg'\n",
    "sqa_output_list = list(Path(sqa_data_path).glob('*_meta.json'))\n",
    "meta_path_sqa = sqa_output_list[0]\n",
    "\n",
    "metadata_list_sqa, data_list_sqa = tsdf.load_metadata_from_path(meta_path_sqa)\n",
    "\n",
    "sync_idx = tsdf_values_idx(metadata_list_sqa, 'sync')\n",
    "data_sync = data_list_sqa[sync_idx]\n",
    "\n",
    "data_sync = data_sync[~(data_sync == 0).all(axis=1)]\n",
    "n_segments_sync = data_sync.shape[0]\n",
    "\n",
    "# Load classification data\n",
    "ppg_prob_idx = tsdf_values_idx(metadata_list_sqa, 'ppg')\n",
    "ppg_post_prob = data_list_sqa[ppg_prob_idx]\n",
    "\n",
    "imu_idx = tsdf_values_idx(metadata_list_sqa, 'sqa_imu')\n",
    "imu_label = data_list_sqa[imu_idx]\n",
    "\n",
    "# Initialize the indices array for classification epochs\n",
    "start_end_indices = np.zeros((len(data_sync[:, 4]), 2), dtype=int)\n",
    "for i in range(len(data_sync[:, 4])):\n",
    "    if i == 0:\n",
    "        start_end_indices[i, 0] = 1\n",
    "    else:\n",
    "        start_end_indices[i, 0] = start_end_indices[i-1, 1] + 1\n",
    "    start_end_indices[i, 1] = np.sum(data_sync[:i+1, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heart Rate Estimation Parameters\n",
    "MIN_WINDOW_LENGTH = 10\n",
    "MIN_HR_SAMPLES = MIN_WINDOW_LENGTH * FS_PPG\n",
    "THRESHOLD_SQA = 0.5\n",
    "HR_EST_LENGTH = 2\n",
    "HR_EST_SAMPLES = HR_EST_LENGTH * FS_PPG\n",
    "\n",
    "# Time-frequency Distribution Parameters\n",
    "TFD_LENGTH = 10\n",
    "KERN_TYPE = 'sep'  # Placeholder type\n",
    "WIN_TYPE_DOPPLER = 'hamm'\n",
    "WIN_TYPE_LAG = 'hamm'\n",
    "WIN_LENGTH_DOPPLER = 1\n",
    "WIN_LENGTH_LAG = 8\n",
    "DOPPLER_SAMPLES = FS_PPG * WIN_LENGTH_DOPPLER\n",
    "LAG_SAMPLES = WIN_LENGTH_LAG * FS_PPG\n",
    "KERN_PARAMS = {'doppler_samples': DOPPLER_SAMPLES, 'win_type_doppler': WIN_TYPE_DOPPLER, 'lag_samples': LAG_SAMPLES, 'win_type_lag': WIN_TYPE_LAG}\n",
    "\n",
    "# Placeholder for moving average filter\n",
    "MA = {\n",
    "    'value': 1,\n",
    "    'window': 30,\n",
    "    'FC': np.ones(30) / 30  # Filter coefficients\n",
    "}\n",
    "\n",
    "v_hr_ppg = []\n",
    "t_hr_unix = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_segments_sync' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Main Loop over all synchronized segments\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mn_segments_sync\u001b[49m):\n\u001b[1;32m      3\u001b[0m     ppg_indices \u001b[38;5;241m=\u001b[39m data_sync[n, :\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m      4\u001b[0m     ppg_segment \u001b[38;5;241m=\u001b[39m data_sync[n, \u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_segments_sync' is not defined"
     ]
    }
   ],
   "source": [
    "# Main Loop over all synchronized segments\n",
    "for n in range(n_segments_sync):\n",
    "    ppg_indices = data_sync[n, :2]\n",
    "    ppg_segment = data_sync[n, 2]\n",
    "\n",
    "    class_start = start_end_indices[n, 0]\n",
    "    class_end = start_end_indices[n, 1]\n",
    "\n",
    "    meta_path_ppg = meta_ppg[ppg_segment]['tsdf_meta_fullpath']\n",
    "    # TODO: Fix loading tsdf\n",
    "    metadata_list_ppg, data_list_ppg = tsdf.load_tsdf_metadata_from_path(meta_path_ppg)\n",
    "\n",
    "    time_idx_ppg = tsdf.get_index(metadata_list_ppg, 'time')\n",
    "    values_idx_ppg = tsdf.get_index(metadata_list_ppg, 'samples')\n",
    "\n",
    "    t_iso_ppg = metadata_list_ppg[time_idx_ppg]['start_iso8601']\n",
    "    datetime_ppg = datetime.strptime(t_iso_ppg, '%d-%b-%Y %H:%M:%S %Z').replace(tzinfo=datetime.timezone.utc)\n",
    "    ts_ppg = datetime_ppg.timestamp() * UNIX_TICKS_MS\n",
    "\n",
    "    t_ppg = np.cumsum(data_list_ppg[time_idx_ppg]) + ts_ppg\n",
    "    tr_ppg = (t_ppg - ts_ppg) / UNIX_TICKS_MS\n",
    "\n",
    "    v_ppg = data_list_ppg[values_idx_ppg]\n",
    "\n",
    "    v_ppg = v_ppg[ppg_indices[0]:ppg_indices[1]]\n",
    "    tr_ppg = tr_ppg[ppg_indices[0]:ppg_indices[1]]\n",
    "\n",
    "    ts_sync = ts_ppg + tr_ppg[0] * UNIX_TICKS_MS\n",
    "    tr_ppg -= tr_ppg[0]\n",
    "\n",
    "    fs_ppg_est = 1 / np.median(np.diff(tr_ppg))\n",
    "\n",
    "    if len(v_ppg) < FS_PPG * MIN_WINDOW_LENGTH:\n",
    "        print('Sample is of insufficient length!')\n",
    "        continue\n",
    "\n",
    "    # Placeholder functions\n",
    "    v_ppg_pre, tr_ppg_pre = preprocessing_ppg(tr_ppg, v_ppg, FS_PPG)\n",
    "    \n",
    "    class_ppg_segment = ppg_post_prob[class_start:class_end]\n",
    "    class_acc_segment = imu_label[class_start:class_end]\n",
    "\n",
    "    # Assign window-level probabilities to individual samples\n",
    "    data_prob_sample = sample_prob_final(class_ppg_segment, class_acc_segment, FS_PPG)\n",
    "\n",
    "    sqa_label = np.zeros(len(data_prob_sample))\n",
    "    sqa_label[data_prob_sample > THRESHOLD_SQA] = 1\n",
    "\n",
    "    v_start_idx, v_end_idx = extract_hr_segments(sqa_label, MIN_HR_SAMPLES)\n",
    "\n",
    "    for i in range(len(v_start_idx)):\n",
    "        if v_start_idx[i] < 2 * FS_PPG or v_end_idx[i] > len(v_ppg_pre) - 2 * FS_PPG:\n",
    "            continue\n",
    "        \n",
    "        rel_ppg = v_ppg_pre[v_start_idx[i]:v_end_idx[i]]\n",
    "        rel_time = tr_ppg_pre[v_start_idx[i]:v_end_idx[i]]\n",
    "\n",
    "        rel_ppg_spwvd = v_ppg_pre[v_start_idx[i] - FS_PPG*2:v_end_idx[i] + FS_PPG*2]\n",
    "        hr_est = PPG_TFD_HR(rel_ppg_spwvd, TFD_LENGTH, MA, FS_PPG, KERN_TYPE, KERN_PARAMS)\n",
    "\n",
    "        if len(rel_ppg) % 60 != 0:\n",
    "            hr_time = rel_time[0:len(rel_ppg) - FS_PPG:HR_EST_SAMPLES]\n",
    "        else:\n",
    "            hr_time = rel_time[0:len(rel_ppg):HR_EST_SAMPLES]\n",
    "\n",
    "        t_epoch_unix = hr_time * UNIX_TICKS_MS + ts_sync\n",
    "        v_hr_ppg.append(hr_est)\n",
    "        t_hr_unix.append(t_epoch_unix)\n",
    "\n",
    "# Save the HR output in TSDF format\n",
    "data_hr_est = [np.array(t_hr_unix) / UNIX_TICKS_MS, np.array(v_hr_ppg)]\n",
    "\n",
    "location = \"../../../tests/data/5.quantification/ppg\"\n",
    "os.makedirs(location, exist_ok=True)\n",
    "\n",
    "if not t_hr_unix:\n",
    "    start_time_iso = datetime.fromtimestamp(0, tz=datetime.timezone.utc).strftime('%d-%b-%Y %H:%M:%S %Z')\n",
    "    end_time_iso = datetime.fromtimestamp(0, tz=datetime.timezone.utc).strftime('%d-%b-%Y %H:%M:%S %Z')\n",
    "else:\n",
    "    start_time_iso = datetime.fromtimestamp(t_hr_unix[0] / UNIX_TICKS_MS, tz=datetime.timezone.utc).strftime('%d-%b-%Y %H:%M:%S %Z')\n",
    "    end_time_iso = datetime.fromtimestamp(t_hr_unix[-1] / UNIX_TICKS_MS, tz=datetime.timezone.utc).strftime('%d-%b-%Y %H:%M:%S %Z')\n",
    "\n",
    "metafile_pre_template = metadata_list_sqa[time_idx_ppg]\n",
    "metafile_pre_template['start_iso8601'] = start_time_iso\n",
    "metafile_pre_template['end_iso8601'] = end_time_iso\n",
    "\n",
    "metafile_time = metafile_pre_template.copy()\n",
    "metafile_values_hr = metafile_pre_template.copy()\n",
    "\n",
    "metafile_time['channels'] = ['time']\n",
    "metafile_time['units'] = ['time_absolute_unix_s']\n",
    "metafile_time['file_name'] = 'hr_est_time.bin'\n",
    "\n",
    "metafile_values_hr['channels'] = ['HR estimates']\n",
    "metafile_values_hr['units'] = ['min^-1']\n",
    "metafile_values_hr['freq_sampling_original'] = round(fs_ppg_est, 2)\n",
    "metafile_values_hr['file_name'] = 'hr_est_values.bin'\n",
    "\n",
    "meta_class = [metafile_time, metafile_values_hr]\n",
    "\n",
    "mat_metadata_file_name = \"hr_est_meta.json\"\n",
    "tsdf.save_tsdf_data(meta_class, data_hr_est, location, mat_metadata_file_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
